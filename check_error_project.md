1. v√©rifier le split + les distributions de labels,
2. traquer les **doublons exacts**,
3. traquer les **quasi-doublons** avec vectorisation TF-IDF (ce qu‚Äôelle demande),
4. v√©rifier les biais de corpus (site/source ‚Üí √©tiquette),
5. faire un sanity check ‚Äúlabels randomis√©s‚Äù pour √©carter un bug du pipeline.

> Hypoth√®ses :
> *dataset_id = `web1`*, *view = `ideology_global`*, colonne texte = `text`, colonne label = `label` (ou `ideology`).
> Si les noms diff√®rent, tu adaptes juste `TEXT_COL` / `LABEL_COL` dans les blocs Python.

---

## 0. Point de d√©part

```bash
cd /chemin/vers/PEPM_M1srbn-project-PEMP_V5.5-CPU-main
source .venv/bin/activate

# (re)g√©n√©rer proprement les TSV
make run STAGE=prepare PROFILE=ideo_quick
```

---

## 1) V√©rifier split + distributions de labels

### 1.1. Comptage de base + d√©tection auto de la colonne label

```bash
python - <<'PY'
from pathlib import Path
import pandas as pd

DATASET_ID = "web1"
VIEW = "ideology_global"

base = Path("data/interim") / DATASET_ID / VIEW
train = pd.read_csv(base / "train.tsv", sep="\t")
job   = pd.read_csv(base / "job.tsv",   sep="\t")

print("train.shape :", train.shape)
print("job.shape   :", job.shape)

# D√©tection na√Øve de la colonne de label
LABEL_CANDIDATES = ["label", "ideology", "y", "target"]
LABEL_COL = None
for c in LABEL_CANDIDATES:
    if c in train.columns:
        LABEL_COL = c
        break

if LABEL_COL is None:
    raise SystemExit(f"Aucune colonne label trouv√©e parmi {LABEL_CANDIDATES}. "
                     f"Colonnes dispo : {list(train.columns)}")

print(f"\n[OK] Colonne de label utilis√©e : {LABEL_COL}\n")

for name, df in [("TRAIN", train), ("JOB", job)]:
    print(f"== {name} ==")
    counts = df[LABEL_COL].value_counts()
    props  = df[LABEL_COL].value_counts(normalize=True).round(3)
    print("Counts :")
    print(counts)
    print("Proportions :")
    print(props)
    print()
PY
```

üëâ √áa te donne :

* nombre de lignes train/job (v√©rifier que √ßa colle avec `TRAIN_PROP=0.6`),
* distribution des labels ‚Üí montre tout de suite si le corpus est tr√®s d√©s√©quilibr√©.

Tu peux d√©j√† calculer √† la main le **baseline majority** : proportion de la classe majoritaire dans `JOB`.

---

## 2) Doublons **exacts** (dans train, dans job, et entre les deux)

> Ici il faut conna√Ætre le nom de la colonne texte. Je pars sur `"text"`.
> Si c‚Äôest `"body"`, `"content"`, `"texte"`, tu modifies `TEXT_COL`.

```bash
python - <<'PY'
from pathlib import Path
import pandas as pd

DATASET_ID = "web1"
VIEW = "ideology_global"
TEXT_COL = "text"   # ‚ö†Ô∏è adapte si besoin
LABEL_CANDIDATES = ["label", "ideology", "y", "target"]

base = Path("data/interim") / DATASET_ID / VIEW
train = pd.read_csv(base / "train.tsv", sep="\t")
job   = pd.read_csv(base / "job.tsv",   sep="\t")

# Label
LABEL_COL = None
for c in LABEL_CANDIDATES:
    if c in train.columns:
        LABEL_COL = c
        break
if LABEL_COL is None:
    raise SystemExit(f"Colonne label introuvable. Colonnes : {list(train.columns)}")

if TEXT_COL not in train.columns:
    raise SystemExit(f"Colonne texte '{TEXT_COL}' absente de train.tsv. "
                     f"Colonnes : {list(train.columns)}")

if TEXT_COL not in job.columns:
    raise SystemExit(f"Colonne texte '{TEXT_COL}' absente de job.tsv. "
                     f"Colonnes : {list(job.columns)}")

print(f"[INFO] LABEL_COL = {LABEL_COL}, TEXT_COL = {TEXT_COL}\n")

# Doublons *dans* chaque split
for name, df in [("TRAIN", train), ("JOB", job)]:
    dup_mask = df.duplicated(TEXT_COL, keep=False)
    n_dup = dup_mask.sum()
    print(f"{name}: {n_dup} doublons exacts (m√™me texte) sur {len(df)} lignes")
    if n_dup:
        print(df.loc[dup_mask, [TEXT_COL, LABEL_COL]].head(5))
        print()

# Doublons *entre* train et job
train_texts = set(train[TEXT_COL])
job_texts   = set(job[TEXT_COL])
overlap = train_texts & job_texts

print(f"\nDoublons exacts TRAIN/JOB (m√™me texte dans les deux) : {len(overlap)}")
if overlap:
    example = next(iter(overlap))
    print("\nExemple de texte en commun (tronqu√©) :")
    print(example[:400].replace("\n", " ") + "...")
PY
```

* Si tu as 0 ou presque ‚Üí pas de fuite triviale par copie stricte.
* S‚Äôil y en a beaucoup ‚Üí tu as un vrai argument ‚Äúle corpus est truff√© de doublons‚Äù.

---

## 3) **Quasi-doublons** avec vectorisation TF-IDF (ce que ta prof veut)

Ici on vectorise et on cherche des paires **tr√®s similaires** (`cosine > 0.9‚Äì0.95`) entre un sous-ensemble de train et job.
On se limite √† ~2000 docs par split pour ne pas exploser la RAM.

```bash
python - <<'PY'
from pathlib import Path
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

DATASET_ID = "web1"
VIEW = "ideology_global"
TEXT_COL = "text"   # adapte si besoin
LABEL_CANDIDATES = ["label", "ideology", "y", "target"]

MAX_PER_SPLIT = 2000
SIM_THRESHOLD = 0.95   # baisse √† 0.9 si tu veux plus large

base = Path("data/interim") / DATASET_ID / VIEW
debug_dir = Path("debug")
debug_dir.mkdir(exist_ok=True)

train = pd.read_csv(base / "train.tsv", sep="\t")
job   = pd.read_csv(base / "job.tsv",   sep="\t")

# Label
LABEL_COL = None
for c in LABEL_CANDIDATES:
    if c in train.columns:
        LABEL_COL = c
        break
if LABEL_COL is None:
    raise SystemExit(f"Colonne label introuvable. Colonnes : {list(train.columns)}")

if TEXT_COL not in train.columns or TEXT_COL not in job.columns:
    raise SystemExit(f"Colonne texte '{TEXT_COL}' absente. "
                     f"train cols={list(train.columns)}, job cols={list(job.columns)}")

print(f"[INFO] LABEL_COL = {LABEL_COL}, TEXT_COL = {TEXT_COL}")

# √âchantillon (pour rester raisonnable en RAM)
train_sub = train.sample(
    n=min(MAX_PER_SPLIT, len(train)),
    random_state=0
).reset_index(drop=True)
job_sub = job.sample(
    n=min(MAX_PER_SPLIT, len(job)),
    random_state=1
).reset_index(drop=True)

print(f"[INFO] train_sub: {len(train_sub)} docs, job_sub: {len(job_sub)} docs")

# TF-IDF (on entra√Æne sur l'union pour partager le vocabulaire)
all_texts = pd.concat([train_sub[TEXT_COL], job_sub[TEXT_COL]], axis=0).fillna("")
vec = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))
X_all = vec.fit_transform(all_texts)

X_train = X_all[:len(train_sub)]
X_job   = X_all[len(train_sub):]

# Similarit√©s job vs train
sim_matrix = (X_job @ X_train.T).toarray()  # (n_job x n_train)

pairs = []
for j_idx in range(sim_matrix.shape[0]):
    row = sim_matrix[j_idx]
    # indices tri√©s par similarit√© d√©croissante
    good = np.where(row >= SIM_THRESHOLD)[0]
    for t_idx in good:
        sim = row[t_idx]
        pairs.append(
            (
                float(sim),
                "job", int(j_idx),
                "train", int(t_idx),
            )
        )

print(f"[INFO] Paires avec sim >= {SIM_THRESHOLD} : {len(pairs)}")

# On garde les meilleures
pairs_sorted = sorted(pairs, key=lambda x: -x[0])[:200]

rows = []
for sim, job_flag, j_idx, train_flag, t_idx in pairs_sorted:
    j_row = job_sub.iloc[j_idx]
    t_row = train_sub.iloc[t_idx]
    rows.append({
        "sim": round(sim, 4),
        "job_index": int(j_idx),
        "train_index": int(t_idx),
        "job_label": j_row[LABEL_COL],
        "train_label": t_row[LABEL_COL],
        "job_text": str(j_row[TEXT_COL])[:400].replace("\n", " "),
        "train_text": str(t_row[TEXT_COL])[:400].replace("\n", " "),
    })

df_pairs = pd.DataFrame(rows)
out_path = debug_dir / "near_duplicates_web1.tsv"
df_pairs.to_csv(out_path, sep="\t", index=False)
print(f"[OUT] {len(df_pairs)} paires quasi-doublons √©crites dans {out_path}")
PY
```

Tu ouvres ensuite `debug/near_duplicates_web1.tsv` dans un tableur ou un √©diteur, et tu montres √† ta prof :

* s‚Äôil y a beaucoup de paires √† 0.98‚Äì1.0 ‚Üí corpus **rempli d‚Äôarticles quasi identiques**,
* si les labels de ces quasi-doublons sont coh√©rents,
* s‚Äôil y a des cas o√π un *m√™me* texte (ou quasi) a deux labels ‚Üí probl√®me d‚Äôannotation.

√áa, c‚Äôest exactement ‚Äúvectoriser pour comparer les articles‚Äù.

---

## 4) V√©rifier les **biais de corpus** (site/source ‚Üí id√©ologie)

Si certains sites sont 100 % gauche ou 100 % droite, le mod√®le peut ‚Äútricher‚Äù en apprenant juste la source.

On regarde les crosstabs pour quelques colonnes m√©tadonn√©es typiques (`corpus_id`, `source`, `site`, `modality`, etc.).

```bash
python - <<'PY'
from pathlib import Path
import pandas as pd

DATASET_ID = "web1"
VIEW = "ideology_global"
LABEL_CANDIDATES = ["label", "ideology", "y", "target"]

base = Path("data/interim") / DATASET_ID / VIEW
job = pd.read_csv(base / "job.tsv", sep="\t")

LABEL_COL = None
for c in LABEL_CANDIDATES:
    if c in job.columns:
        LABEL_COL = c
        break
if LABEL_COL is None:
    raise SystemExit(f"Colonne label introuvable. Colonnes : {list(job.columns)}")

META_CANDIDATES = ["corpus_id", "source", "site", "modality", "media", "channel"]

print(f"[INFO] LABEL_COL = {LABEL_COL}")
print("[INFO] Colonnes meta candidates pr√©sentes :",
      [c for c in META_CANDIDATES if c in job.columns])

for col in META_CANDIDATES:
    if col not in job.columns:
        continue
    print(f"\n=== Crosstab {col} x {LABEL_COL} (normalis√© par ligne) ===")
    ct = pd.crosstab(job[col], job[LABEL_COL], normalize="index").round(3)
    print(ct)
PY
```

* Si tu vois des lignes genre ‚Äúsite_X : 0.99 left / 0.01 right‚Äù,
  ‚Üí tu as un **biais de source √©norme** que tu peux documenter.
* Tu peux aussi montrer que, sans m√™me regarder les textes, un classifieur ‚Äúau pif mais connaissant le site‚Äù aurait d√©j√† de tr√®s bons scores.

C‚Äôest un argument solide pour expliquer des performances ‚Äútrop bonnes‚Äù.

---

## 5) Sanity check ultime : **randomiser les labels** et re-entra√Æner

√áa, c‚Äôest pour tester si ton pipeline est sain : si tu casses texte‚Üílabel, les scores doivent tomber au niveau du hasard.

```bash
# 5.1. Re-g√©n√©rer un train propre
make run STAGE=prepare PROFILE=ideo_quick

# 5.2. Randomiser les labels du train.tsv
python - <<'PY'
from pathlib import Path
import pandas as pd
import numpy as np

DATASET_ID = "web1"
VIEW = "ideology_global"
LABEL_CANDIDATES = ["label", "ideology", "y", "target"]

base = Path("data/interim") / DATASET_ID / VIEW
train_path  = base / "train.tsv"
backup_path = base / "train.original.tsv"

df = pd.read_csv(train_path, sep="\t")

LABEL_COL = None
for c in LABEL_CANDIDATES:
    if c in df.columns:
        LABEL_COL = c
        break
if LABEL_COL is None:
    raise SystemExit(f"Colonne label introuvable. Colonnes : {list(df.columns)}")

print(f"[INFO] LABEL_COL = {LABEL_COL}")

# Backup de l'original (une seule fois)
if not backup_path.exists():
    df.to_csv(backup_path, sep="\t", index=False)
    print(f"[BACKUP] train original sauvegard√© dans {backup_path}")
else:
    print(f"[BACKUP] {backup_path} existe d√©j√† (non r√©√©crit)")

print("\n[AVANT] R√©partition des labels :")
print(df[LABEL_COL].value_counts())

# Randomisation des labels (en gardant la distribution)
rng = np.random.RandomState(42)
shuffled = df[LABEL_COL].sample(frac=1.0, random_state=rng).reset_index(drop=True)
df[LABEL_COL] = shuffled

print("\n[APRES] R√©partition des labels (doit √™tre similaire mais m√©lang√©e) :")
print(df[LABEL_COL].value_counts())

df.to_csv(train_path, sep="\t", index=False)
print(f"\n[WRITE] train.tsv √©cras√© avec labels randomis√©s")
PY

# 5.3. Train + evaluate sur ces donn√©es pourries
make run STAGE=train    PROFILE=ideo_quick
make run STAGE=evaluate PROFILE=ideo_quick
```

Ensuite tu ouvres un `metrics.json` / `classification_report.txt` :

* Si accuracy + macro-F1 tombent vers le **hasard**, ton pipeline d‚Äô√©val est **sain**.
* Si tu restes √† des scores ‚Äúbons‚Äù ‚Üí il y a forc√©ment une fuite ou un bug.

---

## 6) Comment vendre √ßa √† ta prof

Avec **ces blocs** tu peux arriver en disant :

> 1. J‚Äôai v√©rifi√© les splits et les distributions de labels (commande 1).
> 2. J‚Äôai test√© les doublons exacts (commande 2).
> 3. J‚Äôai vectoris√© le corpus et extrait les quasi-doublons (commande 3) ‚Üí fichier `debug/near_duplicates_web1.tsv`.
> 4. J‚Äôai mesur√© les biais de source (commande 4) : certains sites sont quasi mono-id√©ologie.
> 5. J‚Äôai randomis√© les labels (commande 5) : les performances s‚Äôeffondrent ‚Üí donc le pipeline ne triche pas.

Et tu peux conclure ensuite :

* soit ‚Äúle corpus est vraiment tr√®s biais√© / facile (lexique + sources)‚Äù ‚Üí r√©sultats √©lev√©s mais explicables,
* soit tu mets le doigt sur un vrai bug (doublons massifs, labels contradictoires, etc.),
* soit‚Ä¶ oui, tu as vraiment bien boss√©, mais tu peux le d√©montrer proprement
